{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\n"
     ]
    }
   ],
   "source": [
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise I went to the dentist the other day and sure enough I saw an angry one jump out of my dentist s bag within minutes of arriving She hardly even noticed \n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'many', 'squids', 'are', 'jumping', 'out', 'of', 'suitcases', 'these', 'days', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minutes', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization = Breaking text into individual words\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = word_tokenize(cleaned)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n"
     ]
    }
   ],
   "source": [
    "# Stemming = Cop off word prefixes and suffixes ('singing' becomes 'sing')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stemming the words 'go' and 'went' are identified as different words. Also, what`s up with 'mani' and 'hardli'? A lemmatizer will fix these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization = Bring words down to their root forms ('are' becomes 'be')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are after the lemmatization still some verbs like 'went' conjugated? 'part_of_speech'-package will solve that issue! This will tell the lemmatizer what part of speech the word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'a', 'n', 'v', 'v', 'v', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'r', 'v', 'r', 'n', 'v', 'n', 'v', 'r', 'n', 'n', 'r', 'v', 'n', 'n', 'v', 'n', 'n', 'n', 'n', 'a', 'n', 'n', 'a', 'n', 'n', 'v', 'n', 'a', 'n', 'v', 'v', 'n', 'n', 'n', 'n', 'n', 'r', 'n', 'n', 'v', 'n', 'r', 'r', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Define get_part_of_speech\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "# Apply get_part_of_speech\n",
    "# from part_of_speech import get_part_of_speech\n",
    "lemmatized = [lemmatizer.lemmatize(get_part_of_speech(token)) for token in tokenized]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Text\n",
    "Parsing is a stage of NLP concerned with segmenting text based on syntax.\n",
    "<br>\n",
    "<br>\n",
    "__Part-of-speech tagging (POS tagging)__\n",
    "<br>\n",
    "Identifies parts of speech as verbs, nouns or adjectives.\n",
    "<br>\n",
    "<br>\n",
    "__Named entity recognition (NER)__\n",
    "<br>\n",
    "Identifies proper nouns: E.g. 'Natalia' vs. 'Berlin'.\n",
    "<br>\n",
    "<br>\n",
    "__Dependency grammar__\n",
    "<br>\n",
    "Identifies relationship between words in a sentence.\n",
    "<br>\n",
    "<br>\n",
    "__Regex parsing__\n",
    "<br>\n",
    "Combined with POS tagging one can identify specific phrase chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\n"
    }
   ],
   "source": [
    "text = \"So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\n"
    }
   ],
   "source": [
    "# Make dependency parsing on the text\n",
    "import spacy\n",
    "dependency_parser = spacy.load('en_core_web_sm')\n",
    "parsed_text = dependency_parser(text)\n",
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "jumping                \n  _________|________________    \n |   |   squids    out      |  \n |   |     |        |       |   \n |   |    many      of     days\n |   |     |        |       |   \nare  .     So   suitcases these\n\n          go                       \n  ________|____________________     \n |   |    |       |      |  without\n |   |    |       |      |     |    \n |   |    |       |      |   seeing\n |   |    |       |      |     |    \nYou can barely anywhere  .    one  \n\n          went               \n  _________|_________         \n |   |     to        |       \n |   |     |         |        \n |   |  dentist     day      \n |   |     |      ___|____    \n I   .    the   the     other\n\n                   saw                           \n  __________________|_________                    \n |   |   |    |              jump                \n |   |   |    |      _________|__________         \n |   |   |    |     |    |    |         out      \n |   |   |    |     |    |    |          |        \n |   |   |    |     |    |    |          of      \n |   |   |    |     |    |    |          |        \n |   |   |    |     |    |    |         bag      \n |   |   |    |     |    |    |          |        \n |   |   |  enough  |    |    |       dentist    \n |   |   |    |     |    |    |     _____|_____   \n ,   I   .   Sure   an angry one   my          's\n\n    noticed         \n  _____|__________   \nShe  hardly even  . \n\n"
    }
   ],
   "source": [
    "# Depict dependencies\n",
    "from nltk import Tree\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_text.sents:\n",
    "    to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models - Bag-of-Words Approach\n",
    "One can help computers make predictions about a language by training a language model on a corpus.\n",
    "<br>\n",
    "<br>\n",
    "__Language Models__ are probabilistic models of a language. Model gets use to figure out the liklehood that a given sound, letter, word, or phrase will be used.\n",
    "<br>\n",
    "<br>\n",
    "One of the most common (statistical) language models is known as __bag-of-words__. Bag-of-words does not have an order but a tally count of each instance for each word.\n",
    "<br>\n",
    "<br>\n",
    "Bag-of-words can be a excellent way of looking at language when one wants to make prediction concerning topic or sentiment of a text. When grammar and word orderare irrelevant, this is probably a good model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n However, the egg only got larger and larger, and more and more human: when she had come within a few yards of it, she saw that it had eyes and a nose and mouth; and when she had come close to it, she saw clearly that it was HUMPTY DUMPTY himself. It cant be anybody else! she said to herself. Im as certain of it, as if his name were written all over his face.\n\nIt might have been written a hundred times, easily, on that enormous face. Humpty Dumpty was sitting with his legs crossed, like a Turk,\n"
    }
   ],
   "source": [
    "looking_glass_text = \"\"\"\n",
    " However, the egg only got larger and larger, and more and more human: when she had come within a few yards of it, she saw that it had eyes and a nose and mouth; and when she had come close to it, she saw clearly that it was HUMPTY DUMPTY himself. It cant be anybody else! she said to herself. Im as certain of it, as if his name were written all over his face.\n",
    "\n",
    "It might have been written a hundred times, easily, on that enormous face. Humpty Dumpty was sitting with his legs crossed, like a Turk, on the top of a high wallsuch a narrow one that Alice quite wondered how he could keep his balanceand, as his eyes were steadily fixed in the opposite direction, and he didnt take the least notice of her, she thought he must be a stuffed figure after all.\n",
    "\n",
    "And how exactly like an egg he is! she said aloud, standing with her hands ready to catch him, for she was every moment expecting him to fall.\n",
    "\n",
    "Its very provoking, Humpty Dumpty said after a long silence, looking away from Alice as he spoke, to be called an eggVery!\n",
    "\n",
    "I said you looked like an egg, Sir, Alice gently explained. And some eggs are very pretty, you know she added, hoping to turn her remark into a sort of a compliment.\n",
    "\n",
    "Some people, said Humpty Dumpty, looking away from her as usual, have no more sense than a baby!\n",
    "\n",
    "Alice didnt know what to say to this: it wasnt at all like conversation, she thought, as he never said anything to her; in fact, his last remark was evidently addressed to a treeso she stood and softly repeated to herself:\n",
    "\n",
    "     Humpty Dumpty sat on a wall:\n",
    "     Humpty Dumpty had a great fall.\n",
    "     All the Kings horses and all the Kings men\n",
    "     Couldnt put Humpty Dumpty in his place again.\n",
    "\n",
    "That last line is much too long for the poetry, she added, almost out loud, forgetting that Humpty Dumpty would hear her.\n",
    "\n",
    "Dont stand there chattering to yourself like that, Humpty Dumpty said, looking at her for the first time, but tell me your name and your business.\n",
    "\n",
    "My name is Alice, but\n",
    "\n",
    "Its a stupid enough name! Humpty Dumpty interrupted impatiently. What does it mean?\n",
    "\n",
    "Must a name mean something? Alice asked doubtfully.\n",
    "\n",
    "Of course it must, Humpty Dumpty said with a short laugh: my name means the shape I amand a good handsome shape it is, too. With a name like yours, you might be any shape, almost.\n",
    "\n",
    "Why do you sit out here all alone? said Alice, not wishing to begin an argument.\n",
    "\n",
    "Why, because theres nobody with me! cried Humpty Dumpty. Did you think I didnt know the answer to that? Ask another.\n",
    "\n",
    "Dont you think youd be safer down on the ground? Alice went on, not with any idea of making another riddle, but simply in her good-natured anxiety for the queer creature. That wall is so very narrow!\n",
    "\n",
    "What tremendously easy riddles you ask! Humpty Dumpty growled out. Of course I dont think so! Why, if ever I did fall offwhich theres no chance ofbut if I did Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. If I did fall, he went on, The King has promised mewith his very own mouthtoto\n",
    "\n",
    "To send all his horses and all his men, Alice interrupted, rather unwisely.\n",
    "\n",
    "Now I declare thats too bad! Humpty Dumpty cried, breaking into a sudden passion. Youve been listening at doorsand behind treesand down chimneysor you couldnt have known it!\n",
    "\n",
    "I havent, indeed! Alice said very gently. Its in a book.\n",
    "\n",
    "Ah, well! They may write such things in a book, Humpty Dumpty said in a calmer tone. Thats what you call a History of England, that is. Now, take a good look at me! Im one that has spoken to a King, I am: mayhap youll never see such another: and to show you Im not proud, you may shake hands with me! And he grinned almost from ear to ear, as he leant forwards (and as nearly as possible fell off the wall in doing so) and offered Alice his hand. She watched him a little anxiously as she took it. If he smiled much more, the ends of his mouth might meet behind, she thought: and then I dont know what would happen to his head! Im afraid it would come off!\n",
    "\n",
    "Yes, all his horses and all his men, Humpty Dumpty went on. Theyd pick me up again in a minute, they would! However, this conversation is going on a little too fast: lets go back to the last remark but one.\n",
    "\n",
    "Im afraid I cant quite remember it, Alice said very politely.\n",
    "\n",
    "In that case we start fresh, said Humpty Dumpty, and its my turn to choose a subject (He talks about it just as if it was a game! thought Alice.) So heres a question for you. How old did you say you were?\n",
    "\n",
    "Alice made a short calculation, and said Seven years and six months.\n",
    "\n",
    "Wrong! Humpty Dumpty exclaimed triumphantly. You never said a word like it!\n",
    "\n",
    "I though you meant How old are you? Alice explained.\n",
    "\n",
    "If Id meant that, Id have said it, said Humpty Dumpty. \n",
    "\"\"\"\n",
    "print(looking_glass_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'get_part_of_speech' will tell what part of speech the word is\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "however the egg only got larger and larger and more and more human when she had come within a few yards of it she saw that it had eyes and a nose and mouth and when she had come close to it she saw clearly that it was humpty dumpty himself it cant be anybody else she said to herself im as certain of it as if his name were written all over his face it might have been written a hundred times easily on that enormous face humpty dumpty was sitting with his legs crossed like a turk on the top of a h\n"
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_text).lower()\n",
    "print(cleaned[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['however', 'the', 'egg', 'only', 'got', 'larger', 'and', 'larger', 'and', 'more', 'and', 'more', 'human', 'when', 'she', 'had', 'come', 'within', 'a', 'few', 'yards', 'of', 'it', 'she', 'saw', 'that', 'it', 'had', 'eyes', 'and', 'a', 'nose', 'and', 'mouth', 'and', 'when', 'she', 'had', 'come', 'close', 'to', 'it', 'she', 'saw', 'clearly', 'that', 'it', 'was', 'humpty', 'dumpty', 'himself', 'it', 'cant', 'be', 'anybody', 'else', 'she', 'said', 'to', 'herself', 'im', 'as', 'certain', 'of', 'it', 'as', 'if', 'his', 'name', 'were', 'written', 'all', 'over', 'his', 'face', 'it', 'might', 'have', 'been', 'written', 'a', 'hundred', 'times', 'easily', 'on', 'that', 'enormous', 'face', 'humpty', 'dumpty', 'was', 'sitting', 'with', 'his', 'legs', 'crossed', 'like', 'a', 'turk', 'on']\n"
    }
   ],
   "source": [
    "# Tokenization = Breaking text into individual words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = word_tokenize(cleaned)\n",
    "print(tokenized[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['however', 'egg', 'got', 'larger', 'larger', 'human', 'come', 'within', 'yards', 'saw', 'eyes', 'nose', 'mouth', 'come', 'close', 'saw', 'clearly', 'humpty', 'dumpty', 'cant', 'anybody', 'else', 'said', 'im', 'certain', 'name', 'written', 'face', 'might', 'written', 'hundred', 'times', 'easily', 'enormous', 'face', 'humpty', 'dumpty', 'sitting', 'legs', 'crossed', 'like', 'turk', 'top', 'high', 'wallsuch', 'narrow', 'one', 'alice', 'quite', 'wondered', 'could', 'keep', 'balanceand', 'eyes', 'steadily', 'fixed', 'opposite', 'direction', 'didnt', 'take', 'least', 'notice', 'thought', 'must', 'stuffed', 'figure', 'exactly', 'like', 'egg', 'said', 'aloud', 'standing', 'hands', 'ready', 'catch', 'every', 'moment', 'expecting', 'fall', 'provoking', 'humpty', 'dumpty', 'said', 'long', 'silence', 'looking', 'away', 'alice', 'spoke', 'called', 'eggvery', 'said', 'looked', 'like', 'egg', 'sir', 'alice', 'gently', 'explained', 'eggs']\n"
    }
   ],
   "source": [
    "# Stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "print(filtered[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['however', 'egg', 'get', 'large', 'large', 'human', 'come', 'within', 'yard', 'saw', 'eye', 'nose', 'mouth', 'come', 'close', 'saw', 'clearly', 'humpty', 'dumpty', 'cant', 'anybody', 'else', 'say', 'im', 'certain', 'name', 'write', 'face', 'might', 'write', 'hundred', 'time', 'easily', 'enormous', 'face', 'humpty', 'dumpty', 'sit', 'leg', 'cross', 'like', 'turk', 'top', 'high', 'wallsuch', 'narrow', 'one', 'alice', 'quite', 'wonder', 'could', 'keep', 'balanceand', 'eye', 'steadily', 'fix', 'opposite', 'direction', 'didnt', 'take', 'least', 'notice', 'think', 'must', 'stuff', 'figure', 'exactly', 'like', 'egg', 'say', 'aloud', 'stand', 'hand', 'ready', 'catch', 'every', 'moment', 'expect', 'fall', 'provoke', 'humpty', 'dumpty', 'say', 'long', 'silence', 'look', 'away', 'alice', 'speak', 'call', 'eggvery', 'say', 'look', 'like', 'egg', 'sir', 'alice', 'gently', 'explain', 'egg']\n"
    }
   ],
   "source": [
    "# Lemmatization = Bring words down to their root forms ('are' becomes 'be')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "print(normalized[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Counter({'humpty': 19, 'dumpty': 19, 'say': 19, 'alice': 16, 'name': 7, 'like': 7, 'think': 7, 'look': 6, 'im': 5, 'know': 5, 'mean': 5, 'go': 5, 'egg': 4, 'fall': 4, 'king': 4, 'would': 4, 'dont': 4, 'come': 3, 'write': 3, 'might': 3, 'sit': 3, 'one': 3, 'didnt': 3, 'take': 3, 'must': 3, 'stand': 3, 'hand': 3, 'remark': 3, 'never': 3, 'last': 3, 'wall': 3, 'horse': 3, 'men': 3, 'almost': 3, 'ask': 3, 'shape': 3, 'good': 3, 'another': 3, 'however': 2, 'large': 2, 'saw': 2, 'eye': 2, 'mouth': 2, 'cant': 2, 'face': 2, 'time': 2, 'narrow': 2, 'quite': 2, 'could': 2, 'long': 2, 'away': 2, 'speak': 2, 'call': 2, 'gently': 2, 'explain': 2, 'add': 2, 'turn': 2, 'conversation': 2, 'couldnt': 2, 'much': 2, 'interrupt': 2, 'course': 2, 'short': 2, 'laugh': 2, 'there': 2, 'cry': 2, 'make': 2, 'riddle': 2, 'thats': 2, 'behind': 2, 'book': 2, 'may': 2, 'ear': 2, 'little': 2, 'afraid': 2, 'old': 2, 'id': 2, 'get': 1, 'human': 1, 'within': 1, 'yard': 1, 'nose': 1, 'close': 1, 'clearly': 1, 'anybody': 1, 'else': 1, 'certain': 1, 'hundred': 1, 'easily': 1, 'enormous': 1, 'leg': 1, 'cross': 1, 'turk': 1, 'top': 1, 'high': 1, 'wallsuch': 1, 'wonder': 1, 'keep': 1, 'balanceand': 1, 'steadily': 1, 'fix': 1, 'opposite': 1, 'direction': 1, 'least': 1, 'notice': 1, 'stuff': 1, 'figure': 1, 'exactly': 1, 'aloud': 1, 'ready': 1, 'catch': 1, 'every': 1, 'moment': 1, 'expect': 1, 'provoke': 1, 'silence': 1, 'eggvery': 1, 'sir': 1, 'pretty': 1, 'hop': 1, 'sort': 1, 'compliment': 1, 'people': 1, 'usual': 1, 'sense': 1, 'baby': 1, 'wasnt': 1, 'anything': 1, 'fact': 1, 'evidently': 1, 'address': 1, 'treeso': 1, 'softly': 1, 'repeat': 1, 'great': 1, 'put': 1, 'place': 1, 'line': 1, 'poetry': 1, 'loud': 1, 'forget': 1, 'hear': 1, 'chatter': 1, 'first': 1, 'tell': 1, 'business': 1, 'stupid': 1, 'enough': 1, 'impatiently': 1, 'something': 1, 'doubtfully': 1, 'amand': 1, 'handsome': 1, 'alone': 1, 'wish': 1, 'begin': 1, 'argument': 1, 'nobody': 1, 'answer': 1, 'youd': 1, 'safe': 1, 'grind': 1, 'idea': 1, 'simply': 1, 'natured': 1, 'anxiety': 1, 'queer': 1, 'creature': 1, 'tremendously': 1, 'easy': 1, 'growl': 1, 'ever': 1, 'offwhich': 1, 'chance': 1, 'ofbut': 1, 'purse': 1, 'lip': 1, 'solemn': 1, 'grand': 1, 'hardly': 1, 'help': 1, 'promise': 1, 'mewith': 1, 'mouthtoto': 1, 'send': 1, 'rather': 1, 'unwisely': 1, 'declare': 1, 'bad': 1, 'break': 1, 'sudden': 1, 'passion': 1, 'youve': 1, 'listen': 1, 'doorsand': 1, 'treesand': 1, 'chimneysor': 1, 'havent': 1, 'indeed': 1, 'ah': 1, 'well': 1, 'thing': 1, 'calm': 1, 'tone': 1, 'history': 1, 'england': 1, 'mayhap': 1, 'youll': 1, 'see': 1, 'show': 1, 'proud': 1, 'shake': 1, 'grin': 1, 'lean': 1, 'forward': 1, 'nearly': 1, 'possible': 1, 'fell': 1, 'offer': 1, 'watch': 1, 'anxiously': 1, 'smile': 1, 'end': 1, 'meet': 1, 'happen': 1, 'head': 1, 'yes': 1, 'theyd': 1, 'pick': 1, 'minute': 1, 'fast': 1, 'let': 1, 'back': 1, 'remember': 1, 'politely': 1, 'case': 1, 'start': 1, 'fresh': 1, 'choose': 1, 'subject': 1, 'talk': 1, 'game': 1, 'here': 1, 'question': 1, 'calculation': 1, 'seven': 1, 'year': 1, 'six': 1, 'month': 1, 'wrong': 1, 'exclaim': 1, 'triumphantly': 1, 'word': 1, 'though': 1})\n"
    }
   ],
   "source": [
    "# Create bag-of-words out of normalized text\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models - N-Grams and NLM\n",
    "Unlike bag-of-words, n-grams models considers a sequence of some number (n) units and calculates the probability of each unit in a body of language given the preceding sequence of length n.\n",
    "<br>\n",
    "<br>\n",
    "N-gram probabilities with larger (n) values can be impressive at language prediction.\n",
    "<br>\n",
    "<br>\n",
    "But there are a couple problems with the n-gram model:\n",
    "<br>\n",
    "1. How can a model make sense from a sentence with yet unseen words. The same issue also pretains for bag-of-words. Language smoothig can help adjusting probabilities for unknown words.\n",
    "<br>\n",
    "2. For a model that accurately predicts human language patterns, one want to be (n) as large as possible. As the sequence length grows, the number of examples of each sequence within your training corpus shrinks.\n",
    "<br>\n",
    "<br>\n",
    "NLM equals Neural Language Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n CHAPTER I. Looking-Glass house\n\nOne thing was certain, that the white kitten had had nothing to do\n"
    }
   ],
   "source": [
    "# Loadtext dataset\n",
    "from looking_glass import looking_glass_full_text\n",
    "print(looking_glass_full_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "chapter i looking glass house one thing was certain that the white kitten had had nothing to do wit\n"
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "print(cleaned[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['chapter', 'i', 'looking', 'glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', 'it', 'was', 'the', 'black', 'kittens', 'fault', 'entirely', 'for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', 'and', 'bearing', 'it', 'pretty', 'well', 'considering', 'so', 'you', 'see', 'that', 'it', 'couldnt', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief', 'the', 'way', 'dinah', 'washed', 'her', 'childrens', 'faces', 'was', 'this', 'first', 'she', 'held', 'the', 'poor', 'thing', 'down', 'by', 'its', 'ear', 'with', 'one', 'paw', 'and', 'then', 'with', 'the', 'other', 'paw', 'she', 'rubbed', 'its', 'face', 'all']\n"
    }
   ],
   "source": [
    "# Tokenization = Breaking text into individual words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = word_tokenize(cleaned)\n",
    "print(tokenized[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n"
    }
   ],
   "source": [
    "# Change the n value to 3\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "print(looking_glass_trigrams_frequency.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}